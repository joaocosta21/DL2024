{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron and Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 \n",
    "\n",
    "Consider the neural network considered in the first question of the theoretical component of the practical class, with number of units: 4,4,3,3.\n",
    "\n",
    "![](https://drive.google.com/uc?id=1SHUgdosKp6AX8rRAACCZ5nb4kUXreI3g)\n",
    "\n",
    "Assume all units, except the ones in the output layer, use the hyperbolic tangent activation function. \n",
    "\n",
    "Consider the following training example:\n",
    "\n",
    "$\\mathbf{x} =\\begin{bmatrix} 1, 0, 1, 0 \\end{bmatrix}^\\intercal $,   $\\mathbf{y} =\\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "❓ Using the squared error loss do a stochastic gradient descent update, initializing all connection weights and biases to 0.1 and a  learning rate η = 0.1:\n",
    "\n",
    "1. Perform the forward pass\n",
    "2. Compute the loss\n",
    "3. Compute gradients with backpropagation\n",
    "4. Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[1, 0, 1, 0]])\n",
    "labels = np.array([[0, 1, 0]])\n",
    "\n",
    "# First is input size, last is output size.\n",
    "units = [4, 4, 3, 3]\n",
    "\n",
    "# Initialize weights with correct shapes \n",
    "\n",
    "W1 = .1 * np.ones((units[1], units[0]))\n",
    "b1 = .1 * np.ones(units[1])\n",
    "W2 = .1 * np.ones((units[2], units[1]))\n",
    "b2 = .1 * np.ones(units[2])\n",
    "W3 = .1 * np.ones((units[3], units[2]))\n",
    "b3 = .1 * np.ones(units[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16396106 0.16396106 0.16396106]\n"
     ]
    }
   ],
   "source": [
    "# Forward Pass\n",
    "\n",
    "h0 = inputs[0]\n",
    "\n",
    "z1 = W1.dot(h0) + b1\n",
    "h1 = np.tanh(z1)\n",
    "\n",
    "z2 = W2.dot(h1) + b2\n",
    "h2 = np.tanh(z2)\n",
    "\n",
    "z3 = W3.dot(h2) + b3\n",
    "\n",
    "y = labels[0]\n",
    "\n",
    "print(z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37636378397755565\n"
     ]
    }
   ],
   "source": [
    "# Loss\n",
    "\n",
    "loss = .5*(z3 - y).dot(z3 - y)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation\n",
    "\n",
    "grad_z3 = z3 - y  # Grad of loss wrt z3.\n",
    "\n",
    "# Gradient of hidden parameters.\n",
    "grad_W3 = grad_z3[:, None].dot(h2[:, None].T)\n",
    "grad_b3 = grad_z3\n",
    "\n",
    "# Gradient of hidden layer below.\n",
    "grad_h2 = W3.T.dot(grad_z3)\n",
    "\n",
    "# Gradient of hidden layer below before activation.\n",
    "grad_z2 = grad_h2 * (1-h2**2)   # Grad of loss wrt z3.\n",
    "\n",
    "# Gradient of hidden parameters.\n",
    "grad_W2 = grad_z2[:, None].dot(h1[:, None].T)\n",
    "grad_b2 = grad_z2\n",
    "\n",
    "# Gradient of hidden layer below.\n",
    "grad_h1 = W2.T.dot(grad_z2)\n",
    "\n",
    "# Gradient of hidden layer below before activation.\n",
    "grad_z1 = grad_h1 * (1-h1**2)   # Grad of loss wrt z3.\n",
    "\n",
    "# Gradient of hidden parameters.\n",
    "grad_W1 = grad_z1[:, None].dot(h0[:, None].T)\n",
    "grad_b1 = grad_z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Gradients\n",
    "\n",
    "# Gradient updates.\n",
    "eta = 0.1\n",
    "W1 -= eta*grad_W1\n",
    "b1 -= eta*grad_b1\n",
    "W2 -= eta*grad_W2\n",
    "b2 -= eta*grad_b2\n",
    "W3 -= eta*grad_W3\n",
    "b3 -= eta*grad_b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Let's say we were using the same training example but with the following changes:\n",
    "- The output units have a softmax activation function\n",
    "- The error function is cross-entropy\n",
    "\n",
    "Keeping the same initializations and learning rate, adjust your computations to the new changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** We need only to change:  \n",
    "- the output, *i.e.*, $\\hat{y} = softmax(z_3)$ instead of $\\hat{y} = z_3$\n",
    "- the loss computation to $L = -y.log(\\hat{y})$\n",
    "- the gradient of the loss with respect to $z_3$: $\\frac{dL}{dz_3}$\n",
    "\n",
    "All other steps remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "W1 = .1 * np.ones((units[1], units[0]))\n",
    "b1 = .1 * np.ones(units[1])\n",
    "W2 = .1 * np.ones((units[2], units[1]))\n",
    "b2 = .1 * np.ones(units[2])\n",
    "W3 = .1 * np.ones((units[3], units[2]))\n",
    "b3 = .1 * np.ones(units[3])\n",
    "\n",
    "# Forward Pass\n",
    "\n",
    "h0 = inputs[0]\n",
    "\n",
    "z1 = W1.dot(h0) + b1\n",
    "h1 = np.tanh(z1)\n",
    "\n",
    "z2 = W2.dot(h1) + b2\n",
    "h2 = np.tanh(z2)\n",
    "\n",
    "z3 = W3.dot(h2) + b3\n",
    "\n",
    "p = np.exp(z3) / sum(np.exp(z3))\n",
    "y = labels[0]\n",
    "\n",
    "# Loss\n",
    "\n",
    "loss = -y.dot(np.log(p))\n",
    "\n",
    "# Backpropagation\n",
    "\n",
    "grad_z3 = p - y  # Grad of loss wrt p\n",
    "\n",
    "# Gradient of hidden parameters.\n",
    "grad_W3 = grad_z3[:, None].dot(h2[:, None].T)\n",
    "grad_b3 = grad_z3\n",
    "\n",
    "# Gradient of hidden layer below.\n",
    "grad_h2 = W3.T.dot(grad_z3)\n",
    "\n",
    "# Gradient of hidden layer below before activation.\n",
    "grad_z2 = grad_h2 * (1-h2**2)   # Grad of loss wrt z3.\n",
    "\n",
    "# Gradient of hidden parameters.\n",
    "grad_W2 = grad_z2[:, None].dot(h1[:, None].T)\n",
    "grad_b2 = grad_z2\n",
    "\n",
    "# Gradient of hidden layer below.\n",
    "grad_h1 = W2.T.dot(grad_z2)\n",
    "\n",
    "# Gradient of hidden layer below before activation.\n",
    "grad_z1 = grad_h1 * (1-h1**2)   # Grad of loss wrt z3.\n",
    "\n",
    "# Gradient of hidden parameters.\n",
    "grad_W1 = grad_z1[:, None].dot(h0[:, None].T)\n",
    "grad_b1 = grad_z1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Complete functions `forward`, `compute_loss`, `backpropagation` and `update_weights` generalized to perform the same computations as before, but for any MLP architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "x: single observation of shape (n,)\n",
    "weights: list of weight matrices [W1, W2, ...]\n",
    "biases: list of biases matrices [b1, b2, ...]\n",
    "\n",
    "y: final output\n",
    "hiddens: list of computed hidden layers [h1, h2, ...]\n",
    "'''\n",
    "\n",
    "def forward(x, weights, biases):\n",
    "    num_layers = len(weights)\n",
    "    g = np.tanh\n",
    "    hiddens = []\n",
    "    # compute hidden layers\n",
    "    for i in range(num_layers):\n",
    "            h = x if i == 0 else hiddens[i-1]\n",
    "            z = weights[i].dot(h) + biases[i]\n",
    "            if i < num_layers-1:  # Assuming the output layer has no activation.\n",
    "                hiddens.append(g(z))\n",
    "    #compute output\n",
    "    output = z\n",
    "    \n",
    "    return output, hiddens\n",
    "\n",
    "def compute_loss(output, y):\n",
    "    # compute loss\n",
    "    probs = np.exp(output) / np.sum(np.exp(output))\n",
    "    loss = -y.dot(np.log(probs))\n",
    "    \n",
    "    return loss   \n",
    "\n",
    "def backward(x, y, output, hiddens, weights):\n",
    "    num_layers = len(weights)\n",
    "    g = np.tanh\n",
    "    z = output\n",
    "\n",
    "    probs = np.exp(output) / np.sum(np.exp(output))\n",
    "    grad_z = probs - y  \n",
    "    \n",
    "    grad_weights = []\n",
    "    grad_biases = []\n",
    "    \n",
    "    # Backpropagate gradient computations \n",
    "    for i in range(num_layers-1, -1, -1):\n",
    "        \n",
    "        # Gradient of hidden parameters.\n",
    "        h = x if i == 0 else hiddens[i-1]\n",
    "        grad_weights.append(grad_z[:, None].dot(h[:, None].T))\n",
    "        grad_biases.append(grad_z)\n",
    "        \n",
    "        # Gradient of hidden layer below.\n",
    "        grad_h = weights[i].T.dot(grad_z)\n",
    "\n",
    "        # Gradient of hidden layer below before activation.\n",
    "        grad_z = grad_h * (1-h**2)   # Grad of loss wrt z3.\n",
    "\n",
    "    # Making gradient vectors have the correct order\n",
    "    grad_weights.reverse()\n",
    "    grad_biases.reverse()\n",
    "    return grad_weights, grad_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Now we will use the MLP on real data to classify handwritten digits.\n",
    "\n",
    "Data is loaded, split into train and test sets and target is one-hot encoded below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_digits()\n",
    "\n",
    "inputs = data.data  \n",
    "labels = data.target  \n",
    "n, p = np.shape(inputs)\n",
    "n_classes = len(np.unique(labels))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode labels as one-hot vectors.\n",
    "one_hot = np.zeros((np.size(y_train, 0), n_classes))\n",
    "for i in range(np.size(y_train, 0)):\n",
    "    one_hot[i, y_train[i]] = 1\n",
    "y_train_ohe = one_hot\n",
    "one_hot = np.zeros((np.size(y_test, 0), n_classes))\n",
    "for i in range(np.size(y_test, 0)):\n",
    "    one_hot[i, y_test[i]] = 1\n",
    "y_test_ohe = one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Complete function `MLP_train_epoch` using your previously defined functions to compute one epoch of training using SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Outputs:\n",
    "    - weights: list of updated weights\n",
    "    - biases: list of updated \n",
    "    - loss: scalar of total loss (sum for all observations)\n",
    "\n",
    "'''\n",
    "\n",
    "def MLP_train_epoch(inputs, labels, weights, biases):\n",
    "    num_layers = len(weights)\n",
    "    total_loss = 0\n",
    "    # For each observation and target\n",
    "    for x, y in zip(inputs, labels):\n",
    "        # Comoute forward pass\n",
    "        output, hiddens = forward(x, weights, biases)\n",
    "        \n",
    "        # Compute Loss and Update total loss\n",
    "        loss = compute_loss(output, y)\n",
    "        total_loss+=loss\n",
    "        # Compute backpropagation\n",
    "        grad_weights, grad_biases = backward(x, y, output, hiddens, weights)\n",
    "        \n",
    "        # Update weights\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            weights[i] -= eta*grad_weights[i]\n",
    "            biases[i] -= eta*grad_biases[i]\n",
    "            \n",
    "    return weights, biases, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a MLP with a single hidden layer of 50 units and a learning rate of $0.001$. \n",
    "\n",
    "❓ Run 100 epochs of your MLP. Save the loss at each epoch in a list and plot the loss evolution after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjEAAAGdCAYAAADjWSL8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAx/ElEQVR4nO3de3TU9Z3/8dfccyEZEkIyBAKCUqFCvYDloi1YEXVFjrWtVlaKras/q6Isul5q91dOzxb8uVv17LJe6vqTdsXF7Vas21rWWC3Kj6tIKogX1Ci3hAAmMwlJZpKZz++PZL7JAFIyt28mPB/nzMnM9/ueyWc+B83rfL6f7+fjMMYYAQAA5Bin3Q0AAABIBiEGAADkJEIMAADISYQYAACQkwgxAAAgJxFiAABATiLEAACAnESIAQAAOcltdwMyJRaLaf/+/SoqKpLD4bC7OQAA4CQYY9Tc3KzKyko5nSceaxmwIWb//v2qqqqyuxkAACAJe/bs0YgRI05YM2BDTFFRkaSuTiguLra5NQAA4GSEQiFVVVVZf8dPZMCGmPglpOLiYkIMAAA55mSmgjCxFwAA5CRCDAAAyEmEGAAAkJMIMQAAICcRYgAAQE4ixAAAgJxEiAEAADmJEAMAAHISIQYAAOQkQgwAAMhJhBgAAJCTCDEAACAnDdgNIDNl14FmPbd5tyqK83TLjNPtbg4AAKcsRmL6aH+wXc/8v0/125r9djcFAIBTGiGmj7yuri6LdEZtbgkAAKc2Qkwfed3dISYas7klAACc2ggxfeSLh5hOQgwAAHYixPRRfCQmTIgBAMBWhJg+YiQGAID+gRDTR15CDAAA/QIhpo/idyd1xoxiMWNzawAAOHURYvooPhIjcYcSAAB2IsT0Ue8QE+4gxAAAYBdCTB/FLydJUjjKgncAANiFENNHDoeDyb0AAPQDhJgk+FyEGAAA7EaISQJbDwAAYD9CTBKsVXuZ2AsAgG0IMUnwMRIDAIDtCDFJYGIvAAD2I8QkgRADAID9CDFJiK8Vw07WAADYhxCTBGtibyeL3QEAYBdCTBJ8bpckLicBAGAnQkwSWCcGAAD7EWKSwMReAADsR4hJAtsOAABgP0JMEnom9hJiAACwCyEmCT4uJwEAYDtCTBKY2AsAgP0IMUlgYi8AAPYjxCTB6+paJ4Y5MQAA2IcQkwRW7AUAwH59CjHLli3T+eefr6KiIpWXl+uqq67SBx98kFBjjNGSJUtUWVmp/Px8zZw5U++++25CTTgc1sKFC1VWVqbCwkLNnTtXe/fuTahpbGzU/Pnz5ff75ff7NX/+fDU1NSX3LdOMib0AANivTyFm7dq1uu2227Rx40ZVV1ers7NTs2fP1pEjR6yahx56SA8//LCWL1+uLVu2KBAI6JJLLlFzc7NVs2jRIq1evVqrVq3SunXr1NLSojlz5iga7RnZmDdvnmpqarRmzRqtWbNGNTU1mj9/fhq+cuqYEwMAQD9gUtDQ0GAkmbVr1xpjjInFYiYQCJgHH3zQqmlvbzd+v9888cQTxhhjmpqajMfjMatWrbJq9u3bZ5xOp1mzZo0xxpidO3caSWbjxo1WzYYNG4wk8/77759U24LBoJFkgsFgKl/xuJ7fstuMuvd3ZsH/3ZT2zwYA4FTWl7/fKc2JCQaDkqTS0lJJUm1trerr6zV79myrxufzacaMGVq/fr0kaevWrero6Eioqays1IQJE6yaDRs2yO/3a8qUKVbN1KlT5ff7rZqjhcNhhUKhhEemcDkJAAD7JR1ijDFavHixLrzwQk2YMEGSVF9fL0mqqKhIqK2oqLDO1dfXy+v1qqSk5IQ15eXlx/zO8vJyq+Zoy5Yts+bP+P1+VVVVJfvV/iKvixV7AQCwW9Ih5vbbb9c777yj//iP/zjmnMPhSHhtjDnm2NGOrjle/Yk+5/7771cwGLQee/bsOZmvkRSfh5EYAADsllSIWbhwoV566SW9/vrrGjFihHU8EAhI0jGjJQ0NDdboTCAQUCQSUWNj4wlrDhw4cMzvPXjw4DGjPHE+n0/FxcUJj0yJrxNDiAEAwD59CjHGGN1+++164YUX9Nprr2n06NEJ50ePHq1AIKDq6mrrWCQS0dq1azV9+nRJ0qRJk+TxeBJq6urqtGPHDqtm2rRpCgaD2rx5s1WzadMmBYNBq8ZObDsAAID93H0pvu222/Tcc8/pt7/9rYqKiqwRF7/fr/z8fDkcDi1atEhLly7V2LFjNXbsWC1dulQFBQWaN2+eVXvjjTfqrrvu0pAhQ1RaWqq7775bEydO1KxZsyRJ48eP12WXXaabbrpJTz75pCTp5ptv1pw5c3TmmWem8/snhVusAQCwX59CzOOPPy5JmjlzZsLxZ555RjfccIMk6Z577lFbW5tuvfVWNTY2asqUKXrllVdUVFRk1T/yyCNyu9265ppr1NbWposvvlgrVqyQq/syjSStXLlSd9xxh3UX09y5c7V8+fJkvmPa9UzsZcVeAADs4jDGGLsbkQmhUEh+v1/BYDDt82M+Ptiii3++VkV5bm1fcmlaPxsAgFNZX/5+s3dSEuIjMVxOAgDAPoSYJPh6TewdoANZAAD0e4SYJMQn9hojdcYIMQAA2IEQk4R4iJFYtRcAALsQYpIQnxMjMS8GAAC7EGKS4HY55XJ2bX9AiAEAwB6EmCRxhxIAAPYixCSpZ+sBFrwDAMAOhJgkxUNMewcjMQAA2IEQkyQfm0ACAGArQkyS2AQSAAB7EWKSxMReAADsRYhJko+RGAAAbEWISVL8chIr9gIAYA9CTJJ8bpckbrEGAMAuhJgkMbEXAAB7EWKSxMReAADsRYhJEnNiAACwFyEmST5CDAAAtiLEJIk5MQAA2IsQkyQv2w4AAGArQkySGIkBAMBehJgk+bg7CQAAWxFikuTzdC12F+5ksTsAAOxAiEkS68QAAGAvQkySmNgLAIC9CDFJYmIvAAD2IsQkKX45icXuAACwByEmST4PIzEAANiJEJMkRmIAALAXISZJzIkBAMBehJgkcXcSAAD2IsQkycdIDAAAtiLEJMnn7lqxlxADAIA9CDFJil9OYtsBAADsQYhJEtsOAABgL0JMkpjYCwCAvQgxSYqHmI6oUSxmbG4NAACnHkJMkuJ3J0mMxgAAYAdCTJK8vUIMq/YCAJB9hJgkxSf2SkzuBQDADoSYJDkcjp47lLicBABA1hFiUsD+SQAA2IcQkwK2HgAAwD6EmBSwai8AAPYhxKSAy0kAANiHEJMCth4AAMA+hJgUWJeTuDsJAICsI8SkgIm9AADYhxCTgp6JvYQYAACyjRCTAq/bJYmRGAAA7ECISQETewEAsA8hJgU9c2JYJwYAgGwjxKTACjHcnQQAQNYRYlJgTeztIMQAAJBthJgUeBmJAQDANoSYFDCxFwAA+xBiUsA6MQAA2IcQkwJffJ0YLicBAJB1hJgUMLEXAAD7EGJSwMReAADsQ4hJgZfF7gAAsA0hJgU+7k4CAMA2hJgU+DxcTgIAwC6EmBTE14lhYi8AANnX5xDzxhtv6Morr1RlZaUcDodefPHFhPM33HCDHA5HwmPq1KkJNeFwWAsXLlRZWZkKCws1d+5c7d27N6GmsbFR8+fPl9/vl9/v1/z589XU1NTnL5hJTOwFAMA+fQ4xR44c0dlnn63ly5d/Yc1ll12muro66/Hyyy8nnF+0aJFWr16tVatWad26dWppadGcOXMUjfZMkJ03b55qamq0Zs0arVmzRjU1NZo/f35fm5tRPRN7CTEAAGSbu69vuPzyy3X55ZefsMbn8ykQCBz3XDAY1NNPP61///d/16xZsyRJzz77rKqqqvTqq6/q0ksv1Xvvvac1a9Zo48aNmjJliiTpqaee0rRp0/TBBx/ozDPP7GuzM4JtBwAAsE9G5sT86U9/Unl5ub70pS/ppptuUkNDg3Vu69at6ujo0OzZs61jlZWVmjBhgtavXy9J2rBhg/x+vxVgJGnq1Kny+/1WzdHC4bBCoVDCI9N8nq4Ve9l2AACA7Et7iLn88su1cuVKvfbaa/r5z3+uLVu26Bvf+IbC4bAkqb6+Xl6vVyUlJQnvq6ioUH19vVVTXl5+zGeXl5dbNUdbtmyZNX/G7/erqqoqzd/sWNbEXkIMAABZ1+fLSX/Jtddeaz2fMGGCJk+erFGjRun3v/+9rr766i98nzFGDofDet37+RfV9Hb//fdr8eLF1utQKJTxIMNidwAA2Cfjt1gPGzZMo0aN0q5duyRJgUBAkUhEjY2NCXUNDQ2qqKiwag4cOHDMZx08eNCqOZrP51NxcXHCI9N83J0EAIBtMh5iDh8+rD179mjYsGGSpEmTJsnj8ai6utqqqaur044dOzR9+nRJ0rRp0xQMBrV582arZtOmTQoGg1ZNf8DdSQAA2KfPl5NaWlr00UcfWa9ra2tVU1Oj0tJSlZaWasmSJfrWt76lYcOG6dNPP9WPfvQjlZWV6Zvf/KYkye/368Ybb9Rdd92lIUOGqLS0VHfffbcmTpxo3a00fvx4XXbZZbrpppv05JNPSpJuvvlmzZkzp9/cmST1jMTEjNQZjcntYu1AAACypc8h5q233tJFF11kvY7PQ1mwYIEef/xxbd++Xb/61a/U1NSkYcOG6aKLLtLzzz+voqIi6z2PPPKI3G63rrnmGrW1teniiy/WihUr5HK5rJqVK1fqjjvusO5imjt37gnXprFDfCRG6prcS4gBACB7HMYYY3cjMiEUCsnv9ysYDGZsfkxnNKYzHviDJGnb31+ikkJvRn4PAACnir78/WboIAVul1PO7pulmNwLAEB2EWJSxOReAADsQYhJkc/Nqr0AANiBEJOi+EhMmAXvAADIKkJMitgEEgAAexBiUuRjTgwAALYgxKTIy9YDAADYghCTIkZiAACwByEmRT0TewkxAABkEyEmRawTAwCAPQgxKeLuJAAA7EGISZF1OYmJvQAAZBUhJkXxFXsZiQEAILsIMSlixV4AAOxBiEkRE3sBALAHISZFTOwFAMAehJgU+TyEGAAA7ECISZHPxbYDAADYgRCTImtibwchBgCAbCLEpIgNIAEAsAchJkVM7AUAwB6EmBT5PF2L3bEBJAAA2UWISZGXib0AANiCEJOinom9rNgLAEA2EWJSxMReAADsQYhJEdsOAABgD0JMinyEGAAAbEGISZGPy0kAANiCEJMir6v7FmtW7AUAIKsIMSliYi8AAPYgxKSIib0AANiDEJMiJvYCAGAPQkyKel9OMsbY3BoAAE4dhJgUxUOMxP5JAABkEyEmRfG9kyQm9wIAkE2EmBQlhBhGYgAAyBpCTIqcTkfPTtaEGAAAsoYQkwbcZg0AQPYRYtIgHmKY2AsAQPYQYtKAy0kAAGQfISYNetaKidrcEgAATh2EmDTwcTkJAICsI8SkARN7AQDIPkJMGjCxFwCA7CPEpAETewEAyD5CTBrkeVySpLYOJvYCAJAthJg0KCnwSJKaWiM2twQAgFMHISYNSgq9kqTG1g6bWwIAwKmDEJMGpQXdIeYIIzEAAGQLISYN4iMxnxNiAADIGkJMGpRal5MIMQAAZAshJg1KChiJAQAg2wgxaVDKxF4AALKOEJMGJYU9t1hHY8bm1gAAcGogxKRB/HJSzEihNkZjAADIBkJMGnhcThXluSVJnzO5FwCArCDEpIk1L4bJvQAAZAUhJk0Gc4cSAABZRYhJk9Lu/ZNYKwYAgOwgxKRJz6q9TOwFACAbCDFpYu2fxEgMAABZQYhJE/ZPAgAguwgxacLdSQAAZBchJk2s/ZO4nAQAQFYQYtKEkRgAALKrzyHmjTfe0JVXXqnKyko5HA69+OKLCeeNMVqyZIkqKyuVn5+vmTNn6t13302oCYfDWrhwocrKylRYWKi5c+dq7969CTWNjY2aP3++/H6//H6/5s+fr6ampj5/wWwpLYzfYs3dSQAAZEOfQ8yRI0d09tlna/ny5cc9/9BDD+nhhx/W8uXLtWXLFgUCAV1yySVqbm62ahYtWqTVq1dr1apVWrdunVpaWjRnzhxFo1GrZt68eaqpqdGaNWu0Zs0a1dTUaP78+Ul8xeyIX04KtnWoMxqzuTUAAJwCTAokmdWrV1uvY7GYCQQC5sEHH7SOtbe3G7/fb5544gljjDFNTU3G4/GYVatWWTX79u0zTqfTrFmzxhhjzM6dO40ks3HjRqtmw4YNRpJ5//33T6ptwWDQSDLBYDCVr3jSOjqj5rT7fmdG3fs7c7C5PSu/EwCAgaYvf7/TOiemtrZW9fX1mj17tnXM5/NpxowZWr9+vSRp69at6ujoSKiprKzUhAkTrJoNGzbI7/drypQpVs3UqVPl9/utmqOFw2GFQqGERza5XU7587svKTEvBgCAjEtriKmvr5ckVVRUJByvqKiwztXX18vr9aqkpOSENeXl5cd8fnl5uVVztGXLllnzZ/x+v6qqqlL+Pn1Vyv5JAABkTUbuTnI4HAmvjTHHHDva0TXHqz/R59x///0KBoPWY8+ePUm0PDXxBe9YtRcAgMxLa4gJBAKSdMxoSUNDgzU6EwgEFIlE1NjYeMKaAwcOHPP5Bw8ePGaUJ87n86m4uDjhkW3WWjHsnwQAQMalNcSMHj1agUBA1dXV1rFIJKK1a9dq+vTpkqRJkybJ4/Ek1NTV1WnHjh1WzbRp0xQMBrV582arZtOmTQoGg1ZNf9RzmzUjMQAAZJq7r29oaWnRRx99ZL2ura1VTU2NSktLNXLkSC1atEhLly7V2LFjNXbsWC1dulQFBQWaN2+eJMnv9+vGG2/UXXfdpSFDhqi0tFR33323Jk6cqFmzZkmSxo8fr8suu0w33XSTnnzySUnSzTffrDlz5ujMM89Mx/fOiBLmxAAAkDV9DjFvvfWWLrroIuv14sWLJUkLFizQihUrdM8996itrU233nqrGhsbNWXKFL3yyisqKiqy3vPII4/I7XbrmmuuUVtbmy6++GKtWLFCLpfLqlm5cqXuuOMO6y6muXPnfuHaNP1FCav2AgCQNQ5jjLG7EZkQCoXk9/sVDAazNj/mP7fs0T2/eUczzxyqFd//alZ+JwAAA0lf/n6zd1IaMRIDAED2EGLSKD6xl52sAQDIPEJMGsUn9jZyizUAABlHiEmj0u7LSS3hToU7o3+hGgAApIIQk0bFeR45uxcUbmplNAYAgEwixKSR0+lgrRgAALKEEJNm3KEEAEB2EGLSzNrJmjuUAADIKEJMmpVY+ycxJwYAgEwixKRZKZeTAADICkJMmjGxFwCA7CDEpJk1EsOcGAAAMooQk2aMxAAAkB2EmDTrmdhLiAEAIJMIMWnG/kkAAGQHISbN4nNiuJwEAEBmEWLSLL5ib1tHVG0RNoEEACBTCDFpVuRzy929CyTzYgAAyBxCTJo5HA5rNIZLSgAAZA4hJgPi+ycxEgMAQOYQYjIgfps1IzEAAGQOISYD2D8JAIDMI8RkgLVqLztZAwCQMYSYDGAkBgCAzCPEZEDPSAwhBgCATCHEZAAjMQAAZB4hJgNYJwYAgMwjxGRAfJ0YQgwAAJlDiMmAYYPzJEkHW8LsnwQAQIYQYjJgSKFXxXluGSPVHjpid3MAABiQCDEZ4HA4dHr5IEnSJ4dabG4NAAADEyEmQ8aUdYeYg4zEAACQCYSYDDm9vFCS9PFBRmIAAMgEQkyGMBIDAEBmEWIy5IzukZhPDrbIGGNzawAAGHgIMRkysrRQLqdDRyJRHQiF7W4OAAADDiEmQ7xup6pK8iV1jcYAAID0IsRk0OlDu+bFMLkXAID0I8Rk0Jih8TuUmNwLAEC6EWIyKD4S8wmr9gIAkHaEmAwaE7+c1MDlJAAA0o0Qk0Hxy0n7g21sBAkAQJoRYjJoSKFX/nwPG0ECAJABhJgMcjgc1mgMG0ECAJBehJgMsyb3cocSAABpRYjJsJ7brBmJAQAgnQgxGcZIDAAAmUGIybDTh7IRJAAAmUCIyTA2ggQAIDMIMRnmdTs1srRAEhtBAgCQToSYLBhTxuReAADSjRCTBaeXx3ezZnIvAADpQojJgvhIDBtBAgCQPoSYLGAjSAAA0o8QkwWn99oIsr2DjSABAEgHQkwWlLIRJAAAaUeIyQKHw2GNxnCHEgAA6UGIyZIzuu9Qeq8uZHNLAAAYGAgxWTJ5VKkkaeMnn9vcEgAABgZCTJZMO32IJOnPe5rUEu60uTUAAOQ+QkyWVJUWqKo0X50xoy21jMYAAJAqQkwWXXB6mSRp/ceHbG4JAAC5jxCTRfFLSus/PmxzSwAAyH2EmCyKh5iddSE1HonY3BoAAHIbISaLyovyNLZ8kIyRNtUyGgMAQCrSHmKWLFkih8OR8AgEAtZ5Y4yWLFmiyspK5efna+bMmXr33XcTPiMcDmvhwoUqKytTYWGh5s6dq71796a7qbaYziUlAADSIiMjMWeddZbq6uqsx/bt261zDz30kB5++GEtX75cW7ZsUSAQ0CWXXKLm5marZtGiRVq9erVWrVqldevWqaWlRXPmzFE0mvv7Dk2zJvcSYgAASEVGQozb7VYgELAeQ4cOldQ1CvPoo4/qgQce0NVXX60JEybol7/8pVpbW/Xcc89JkoLBoJ5++mn9/Oc/16xZs3Tuuefq2Wef1fbt2/Xqq69morlZNXVMqRwO6aOGFjWE2u1uDgAAOSsjIWbXrl2qrKzU6NGj9d3vfleffPKJJKm2tlb19fWaPXu2Vevz+TRjxgytX79ekrR161Z1dHQk1FRWVmrChAlWTS4bXODVWZXFkqQNnzAaAwBAstIeYqZMmaJf/epX+p//+R899dRTqq+v1/Tp03X48GHV19dLkioqKhLeU1FRYZ2rr6+X1+tVSUnJF9YcTzgcVigUSnj0V9Pjl5Q+IsQAAJCstIeYyy+/XN/61rc0ceJEzZo1S7///e8lSb/85S+tGofDkfAeY8wxx472l2qWLVsmv99vPaqqqlL4FpllrRfzCYveAQCQrIzfYl1YWKiJEydq165d1l1KR4+oNDQ0WKMzgUBAkUhEjY2NX1hzPPfff7+CwaD12LNnT5q/Sfqcf1qp3E6H9nzepj2ft9rdHAAAclLGQ0w4HNZ7772nYcOGafTo0QoEAqqurrbORyIRrV27VtOnT5ckTZo0SR6PJ6Gmrq5OO3bssGqOx+fzqbi4OOHRXw3yuXV21WBJbEEAAECy0h5i7r77bq1du1a1tbXatGmTvv3tbysUCmnBggVyOBxatGiRli5dqtWrV2vHjh264YYbVFBQoHnz5kmS/H6/brzxRt1111364x//qG3btun666+3Lk8NFKwXAwBAatzp/sC9e/fquuuu06FDhzR06FBNnTpVGzdu1KhRoyRJ99xzj9ra2nTrrbeqsbFRU6ZM0SuvvKKioiLrMx555BG53W5dc801amtr08UXX6wVK1bI5XKlu7m2mXb6EP3Lax9p/ceHFYsZOZ0nnhMEAAASOYwxxu5GZEIoFJLf71cwGOyXl5baO6Ka/A+vqiXcqVU3T9XUMUPsbhIAALbry99v9k6ySZ7HpSvPHiZJ+s+3+u8kZAAA+itCjI2+M7nrNvCXt9cp1N5hc2sAAMgthBgbnVs1WGeUD1J7R0y/+3Od3c0BACCnEGJs5HA4dM3kEZK4pAQAQF8RYmz2zXNHyO10qGZPkz480PyX3wAAACQRYmw3tMinb4wrlyT9mtEYAABOGiGmH7ime4LvC2/vU0c0ZnNrAADIDYSYfmDmmUM1tMinw0ci+uN7DXY3BwCAnECI6QfcLqeuPm+4JC4pAQBwsggx/cR3JnVdUnr9gwYdCLXb3BoAAPo/Qkw/cUb5IE0aVaKYYTQGAICTQYjpR66fOlKS9G/ralnBFwCAv4AQ04/MPXu4xpYPUlNrh5564xO7mwMAQL9GiOlHXE6H7pp9piTp6XW1OtgctrlFAAD0X4SYfubSsyp0dtVgtUai+tfXP7K7OQAA9FuEmH7G4XDo3ku7RmNWbvpMez5vtblFAAD0T4SYfmj6GWW68IwydUSNHn11l93NAQCgXyLE9FN/1z0a88K2vWwMCQDAcRBi+qmzqwbrsrMCMkb6p//5wO7mAADQ7xBi+rG7L/2SnA7plZ0H9Nr7B+xuDgAA/Qohph87o7xIP7hgtCTpnv96R4dauOUaAIA4Qkw/d/elZ2pcoEiHWiK657/ekTHG7iYBANAvEGL6uTyPS49+9xx53U699n6Dnt202+4mAQDQLxBicsC4QLHuvWycJOlnv9+pjxpabG4RAAD2I8TkiO9PP01fG1um9o6YFj2/TZHOmN1NAgDAVoSYHOF0OvRP3zlbJQUe7dgX0tKX37O7SQAA2IoQk0MqivP04Le+Iklasf5TPbn2Y5tbBACAfQgxOebSswJ64K/GS5KW/eF9/dfWvTa3CAAAexBictBNXx+jm78+RpJ072/eYSE8AMApiRCTo+67bJyuPne4ojGjW1e+ra2fNdrdJAAAsooQk6OcTof+z7e/oplnDlV7R0w/WLFFf97TZHezAADIGkJMDvO4nHrsr8/TeSMHK9jWoeue2qg/fdBgd7MAAMgKQkyOK/C69asbp+hrY8vUGonqb375ll54m8m+AICBjxAzAAzyufX0gvN11TmV6owZLf7PP+vJtR+zzxIAYEAjxAwQXrdTD19zjm76Wteu18v+8L7+/rc7FO6M2twyAAAygxAzgDidDj1wxZf14yu61pF5duNuffvxDdp9uNXmlgEAkH6EmAHob742Rs/ccL5KCjzavi+oK/75Tb28vc7uZgEAkFaEmAHqonHl+v0dX9PkUSVqDnfq1pVv63//dofaIlxeAgAMDISYAaxycL7+4+ap+uHM0yVJv9rwmS55ZK1ef5/bsAEAuY8QM8B5XE7de9k4rfj++ar052lvY5u+v2KLfvjsVtUF2+xuHgAASSPEnCJmnlmu6sUz9L++PkYup0N/2FGvWT9fqyfWfqz2Di4xAQByj8MM0MVEQqGQ/H6/gsGgiouL7W5Ov/JeXUg/fnGHtd9SeZFPC79xhq49f6S8bnItAMA+ffn7TYg5RcViRi9s26dHqj/Uvqauy0rDB+frzllj9c1zh8vjIswAALKPECNCzMmKdMb0/Jbd+pfXPlJDc1iSVOnP0w0XnKZrzx8pf77H5hYCAE4lhBgRYvqqLRLVv2/8VL94o1aHWrrCTKHXpWvOr9L3p4/WyCEFNrcQAHAqIMSIEJOs9o6oXqrZr39b94k+PNAiSXI4pAvPKNN1Xx2pWeMrmDcDAMgYQowIMakyxuiNXYf0b29+ojd3HbKODyn06tuTRuhbk0boSxVFNrYQADAQEWJEiEmn3Ydb9fxbu/Xrt/Za82YkaVygSHPPqdSVX6lUVSmXmwAAqSPEiBCTCZ3RmF57v0H/+dZerf2wQR3Rnn86544crNlfDuiSL5fr9KGD5HA4bGwpACBXEWJEiMm0YGuH/rCjTr+t2a+NtYfV+1/R6LJCzRpfrplnlmvSqBLleVz2NRQAkFMIMSLEZNOBULte2XlAr+48oA0fH1YkGrPO5XmcOv+0Un197FBdcEaZxgWK5HQySgMAOD5CjAgxdmkJd+qNDw/q1fcO6M1dh3Sw1xwaSfLne3T+aaWaMrpUXx1dqi9XFrOwHgDAQogRIaY/MMbowwMtenPXQb2565C2fPq5WiOJ+zTle1yaOMKvc0cO1nkjS3TuyMEqL8qzqcUAALsRYkSI6Y86ojG9uz+kzbWHtbn2c22u/Vyh9s5j6gLFeZow3K+vjPBr4nC/zhpeTLABgFMEIUaEmFwQixl9cqhFb3/WpG17GrVtd5M+ONCs4/2LLBvk1bhAscYPK9L4YcU6o3yQTh86SIU+d/YbDgDIGEKMCDG56ki4U+/uD2n7vqC2723S9n1BfXLoyHGDjdS1z9Pp3YFmdFmhTisr1OghhRpeki8XE4gBIOcQYkSIGUjaIlF9cKBZ79WF9H5dSO/VN+vjhhYdPhL5wvd4XA6NKClQVWmBqkryNbK06/nwwfkaUZKv0kIva9kAQD/Ul7/fjMWj38v3unRO1WCdUzU44XjjkYg+PtiijxpaVHvoiGoPHdGnh4/o08OtinTGrGPH/UyPS5WD81Q5OF+V/nwN6/U84PepojhPRXns4A0A/RkjMRhwYjGjulC79nzeqt2ft2rP56367HCr9ja2am9jW8LWCSdS6HUp4M9TRXGeyot8Ku/1s2yQV+VFPpUN8smf72FUBwDShMtJIsTgi4U7o6pratfexjbtD7aprqlddcE27Q+2q66pTfWhdjUf566pL+J1OVVa6NWQQV4NGeRTWaFXpYVelQ7yqrSg+3mhVyWFXpUUeOXP9zBfBwC+AJeTgBPwuV06rXsS8BdpjXSqPtiu+lC7GkJhNTS360AorIbmsBpC7TrUEtbB5rBC7Z2KRGOqD3XVngyHo2vRv5ICrwYXeDQ436PB3c/9+T2P4jyP/AUeFeW5VZzX9bPQ62bFYwDoRogBjqPA69aYoYM0ZuigE9aFO6M61BLR4ZawDrdEdKglrMNHul5/fqRDja0RHT4S0edHwmo60qHmcKeMkZpaO9TU2tHndjkc0iCfW0U+t4ryPBqU59YgX8+j0OfWIJ9Lhd3PC30uFXi7zuV7XSr0ulXgdanA21Xjczu5FAYgZxFigBT43C4NH5yv4YPzT6q+IxpTU2tXuGk8ElFTW4eCrR1qaouosbVDwbauR6j7EWzrUHN7p0LtHeqIGhkjNbd3dl3uCp7cyM+JOBxSgcel/F7hJs/T9TPf41L+UT99nq6feR5n98/4wymfu+tn/JjP7ZTP7bSeu9leAkCaEWKALPK4nBpa5NPQIl+f3meMUbgzplB7h0JtnToS7lRLuLM70HToSLhTRyJRtYR7zh0Jd6q117HWSLT70an2jlj350pHIlEdOWo7iExwOR3Kczvl6xVwvO6u8ON1O+V1OeXzdP309j7nclivva6uWo/LIZ/bKU+vWo+r570eV1eNdd7llNvlkNfVdc4dP+dycnkOyGGEGCAHOBwOa4SjvCj1z4vGjFojnWqzgk1UbR2daovEuo53RK1z7Z1RtUeiauuIqr0jZh0Ld/Qca++Idj9iCnfGFO6IKtwZS9jRPBozWQtMfeFyOuR2Oqyg43b1hB6309EdiLqDj7OnxuN0WGHI43LK7ew+7nLI7ez66XL2ro2fc8jV65i7+3Pczp7nvT+v93mX09Hzuc6eNsZfx78LwQynCkIMcApyOR0qyvNkfC2cWKxrBKm9I6pINKZwR0zhzmjPsc7u0NPZdTzSHXzixyPdj45ozApF8de9j/e8NtbzSLTreEfUWLUd0ZhiR92PGY0ZRbvbOVA4HDom3FgPR8/zeOBxOx1yOrrCktOReNzlTDwWf3/Xcx3nWM97XM7E8w5H/LmOqXU6ZH2Gs7uu67l61XS912Gd7/W8+/Vx677gXPz3OLq/h9PRU9P7uaP7vVZt7/MERlv1+xDz2GOP6R//8R9VV1ens846S48++qi+9rWv2d0sACfB6XR0zafxuuxuiiUaM1ag6YyHnu6w09n9PH68I2rUGTv2dby2I2bU0Rnr+szuuvjxzmhMnTHTdeyo90R7H+t1rqttJuHzorGumoRz3bWdRyeybsaoO9AZSQMnnPVXCYGn+/nRP53W694BqTtMORPf79CxnxevcTgc3ed7auRQr88/ts7R+3c5JIe6A6C6f1/v9qnrtaP7M+O1UvwzEuvOKB+k66eOsq3v+3WIef7557Vo0SI99thjuuCCC/Tkk0/q8ssv186dOzVy5Ei7mwcgB3WNQnRdmst1xnSFmqgxVqiJB5+jw0/MdJ2LGdM9ImUUjUmdsZhi8Z+9aqK9jkVjXaNqnd2/K9b9+bFev7/rmHqe96qLP48Z/cXj1u8zPZ8fM0ax2FE1puf7x4x6He+aAB9/rzHq+Yz4e7o/x8R/b/fnG/V8Xl/Eun+fNCCXXTuhGV8aamuI6deL3U2ZMkXnnXeeHn/8cevY+PHjddVVV2nZsmUnfC+L3QEAkhWLGRkdFYaMkekdhnrVHB2c4qEp1v2ermM9NdKJa3r//vjvi/WqOea1esKcUeJnqff57s9TwufGQ5iO+UxzVE38dbxtpw0p1HcmV6W17wfEYneRSERbt27Vfffdl3B89uzZWr9+/TH14XBY4XDPcvKhUCjjbQQADEzxuS4uOTQABu0GrH67cMOhQ4cUjUZVUVGRcLyiokL19fXH1C9btkx+v996VFWlNxkCAID+pd+GmLijVxM1xhx3hdH7779fwWDQeuzZsydbTQQAADbot5eTysrK5HK5jhl1aWhoOGZ0RpJ8Pp98vr4tIAYAAHJXvx2J8Xq9mjRpkqqrqxOOV1dXa/r06Ta1CgAA9Bf9diRGkhYvXqz58+dr8uTJmjZtmn7xi19o9+7duuWWW+xuGgAAsFm/DjHXXnutDh8+rJ/+9Keqq6vThAkT9PLLL2vUKPvuSQcAAP1Dv14nJhWsEwMAQO7py9/vfjsnBgAA4EQIMQAAICcRYgAAQE4ixAAAgJxEiAEAADmJEAMAAHJSv14nJhXxO8fZzRoAgNwR/7t9MivADNgQ09zcLEnsZg0AQA5qbm6W3+8/Yc2AXewuFotp//79KioqOu6u16kIhUKqqqrSnj17WEgvw+jr7KGvs4e+zh76OnvS1dfGGDU3N6uyslJO54lnvQzYkRin06kRI0Zk9HcUFxfzH0WW0NfZQ19nD32dPfR19qSjr//SCEwcE3sBAEBOIsQAAICcRIhJgs/n009+8hP5fD67mzLg0dfZQ19nD32dPfR19tjR1wN2Yi8AABjYGIkBAAA5iRADAAByEiEGAADkJEIMAADISYSYPnrsscc0evRo5eXladKkSXrzzTftblLOW7Zsmc4//3wVFRWpvLxcV111lT744IOEGmOMlixZosrKSuXn52vmzJl69913bWrxwLFs2TI5HA4tWrTIOkZfp8++fft0/fXXa8iQISooKNA555yjrVu3Wufp6/To7OzUj3/8Y40ePVr5+fkaM2aMfvrTnyoWi1k19HVy3njjDV155ZWqrKyUw+HQiy++mHD+ZPo1HA5r4cKFKisrU2FhoebOnau9e/emp4EGJ23VqlXG4/GYp556yuzcudPceeedprCw0Hz22Wd2Ny2nXXrppeaZZ54xO3bsMDU1NeaKK64wI0eONC0tLVbNgw8+aIqKisxvfvMbs337dnPttdeaYcOGmVAoZGPLc9vmzZvNaaedZr7yla+YO++80zpOX6fH559/bkaNGmVuuOEGs2nTJlNbW2teffVV89FHH1k19HV6/MM//IMZMmSI+d3vfmdqa2vNr3/9azNo0CDz6KOPWjX0dXJefvll88ADD5jf/OY3RpJZvXp1wvmT6ddbbrnFDB8+3FRXV5u3337bXHTRRebss882nZ2dKbePENMHX/3qV80tt9yScGzcuHHmvvvus6lFA1NDQ4ORZNauXWuMMSYWi5lAIGAefPBBq6a9vd34/X7zxBNP2NXMnNbc3GzGjh1rqqurzYwZM6wQQ1+nz7333msuvPDCLzxPX6fPFVdcYX7wgx8kHLv66qvN9ddfb4yhr9Pl6BBzMv3a1NRkPB6PWbVqlVWzb98+43Q6zZo1a1JuE5eTTlIkEtHWrVs1e/bshOOzZ8/W+vXrbWrVwBQMBiVJpaWlkqTa2lrV19cn9L3P59OMGTPo+yTddtttuuKKKzRr1qyE4/R1+rz00kuaPHmyvvOd76i8vFznnnuunnrqKes8fZ0+F154of74xz/qww8/lCT9+c9/1rp16/RXf/VXkujrTDmZft26das6OjoSaiorKzVhwoS09P2A3QAy3Q4dOqRoNKqKioqE4xUVFaqvr7epVQOPMUaLFy/WhRdeqAkTJkiS1b/H6/vPPvss623MdatWrdLbb7+tLVu2HHOOvk6fTz75RI8//rgWL16sH/3oR9q8ebPuuOMO+Xw+fe9736Ov0+jee+9VMBjUuHHj5HK5FI1G9bOf/UzXXXedJP5dZ8rJ9Gt9fb28Xq9KSkqOqUnH305CTB85HI6E18aYY44hebfffrveeecdrVu37phz9H3q9uzZozvvvFOvvPKK8vLyvrCOvk5dLBbT5MmTtXTpUknSueeeq3fffVePP/64vve971l19HXqnn/+eT377LN67rnndNZZZ6mmpkaLFi1SZWWlFixYYNXR15mRTL+mq++5nHSSysrK5HK5jkmODQ0Nx6RQJGfhwoV66aWX9Prrr2vEiBHW8UAgIEn0fRps3bpVDQ0NmjRpktxut9xut9auXat//ud/ltvttvqTvk7dsGHD9OUvfznh2Pjx47V7925J/LtOp7/7u7/Tfffdp+9+97uaOHGi5s+fr7/927/VsmXLJNHXmXIy/RoIBBSJRNTY2PiFNakgxJwkr9erSZMmqbq6OuF4dXW1pk+fblOrBgZjjG6//Xa98MILeu211zR69OiE86NHj1YgEEjo+0gkorVr19L3fXTxxRdr+/btqqmpsR6TJ0/WX//1X6umpkZjxoyhr9PkggsuOGapgA8//FCjRo2SxL/rdGptbZXTmfjnzOVyWbdY09eZcTL9OmnSJHk8noSauro67dixIz19n/LU4FNI/Bbrp59+2uzcudMsWrTIFBYWmk8//dTupuW0H/7wh8bv95s//elPpq6uznq0trZaNQ8++KDx+/3mhRdeMNu3bzfXXXcdt0emSe+7k4yhr9Nl8+bNxu12m5/97Gdm165dZuXKlaagoMA8++yzVg19nR4LFiwww4cPt26xfuGFF0xZWZm55557rBr6OjnNzc1m27ZtZtu2bUaSefjhh822bduspUVOpl9vueUWM2LECPPqq6+at99+23zjG9/gFmu7/Ou//qsZNWqU8Xq95rzzzrNuA0byJB338cwzz1g1sVjM/OQnPzGBQMD4fD7z9a9/3Wzfvt2+Rg8gR4cY+jp9/vu//9tMmDDB+Hw+M27cOPOLX/wi4Tx9nR6hUMjceeedZuTIkSYvL8+MGTPGPPDAAyYcDls19HVyXn/99eP+/3nBggXGmJPr17a2NnP77beb0tJSk5+fb+bMmWN2796dlvY5jDEm9fEcAACA7GJODAAAyEmEGAAAkJMIMQAAICcRYgAAQE4ixAAAgJxEiAEAADmJEAMAAHISIQYAAOQkQgwAAMhJhBgAAJCTCDEAACAnEWIAAEBO+v8pIPHyoZWkcwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize weights\n",
    "\n",
    "# Single hidden unit MLP with 50 hidden units.\n",
    "# First is input size, last is output size.\n",
    "units = [64, 50, 10]\n",
    "\n",
    "# Initialize all weights and biases randomly.\n",
    "W1 = .1 * np.random.randn(units[1], units[0])\n",
    "b1 = .1 * np.random.randn(units[1])\n",
    "W2 = .1 * np.random.randn(units[2], units[1])\n",
    "b2 = .1 * np.random.randn(units[2])\n",
    "\n",
    "weights = [W1, W2]\n",
    "biases = [b1, b2]\n",
    "\n",
    "# Learning rate.\n",
    "eta = 0.001  \n",
    "    \n",
    "# Run epochs\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    weights, biases, loss = MLP_train_epoch(X_train, y_train_ohe, weights, biases)\n",
    "    losses.append(loss)\n",
    "    \n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Complete function `MLP_predict` to get array of predictions from your trained MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_predict(inputs, weights, biases):\n",
    "    predicted_labels = []\n",
    "    for x in inputs:\n",
    "        # Compute forward pass and get the class with the highest probability\n",
    "        output, _ = forward(x, weights, biases)\n",
    "        y_hat = np.argmax(output)\n",
    "        predicted_labels.append(y_hat)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Compute the accuracy on the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = MLP_predict(X_train, weights, biases)\n",
    "y_test_pred = MLP_predict(X_test, weights, biases)\n",
    "\n",
    "print(f'Train accuracy: {(y_train_pred==y_train).mean()}')\n",
    "print(f'Test accuracy: {(y_test_pred==y_test).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare our results with Sklearn's implementation of the MLP. Compare their accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9993041057759221\n",
      "0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(50),\n",
    "                    activation='tanh',\n",
    "                    solver='sgd',\n",
    "                    learning_rate='constant',\n",
    "                    learning_rate_init=0.001,\n",
    "                    nesterovs_momentum=False,\n",
    "                    random_state=1,\n",
    "                    max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
