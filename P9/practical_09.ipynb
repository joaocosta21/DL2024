{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94ce8f61",
   "metadata": {},
   "source": [
    "# Practical 09 - Recurrent Neural Networks\n",
    "\n",
    "**Disclamer**: Some material from the suggested exercises were retrieved and adapted from the following sources:\n",
    "* [Coursera Sequence Models course by DeepLearningAI](https://www.coursera.org/learn/nlp-sequence-models)\n",
    "* [Pytorch Tutorial for RNNs](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html)\n",
    "* [Pytorch Sentiment Analysis Tutorial](https://github.com/bentrevett/pytorch-sentiment-analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c787d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c74d5",
   "metadata": {},
   "source": [
    "## Question 01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7dd504",
   "metadata": {},
   "source": [
    "Perform a forward propagation for \n",
    "\n",
    "$ x^{(1)} = \\begin{bmatrix} \\begin{pmatrix}\n",
    "4 \\\\ 0 \\\\ 0 \\\\ 0\n",
    "\\end{pmatrix}, \n",
    " \\begin{pmatrix}\n",
    "0 \\\\ 8 \\\\ 2 \\\\ 0\n",
    "\\end{pmatrix}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "With targets:\n",
    "\n",
    "$ t^{(1)} = \\begin{bmatrix} \\begin{pmatrix}\n",
    "1 \\\\ 0\n",
    "\\end{pmatrix}, \n",
    " \\begin{pmatrix}\n",
    "0 \\\\ 1\n",
    "\\end{pmatrix}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Initialize all weights and biases to 0.1, using 3 units per hidden layer, initializing the hidden state to all zeros and using η = 1.0.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a42d2ad",
   "metadata": {},
   "source": [
    "A Recurrent neural network can be seen as the repetition of a single cell. We are first going to implement the computations for a single time-step. The following figure describes the operations for a single time-step of an RNN cell. \n",
    "\n",
    "<img src=\"forward-net.png\" style=\"width:700px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1c935e",
   "metadata": {},
   "source": [
    "\n",
    "**Steps to code the RNN cell**:\n",
    "1. Compute the hidden state with tanh activation: $a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a)$.\n",
    "2. Using your new hidden state $a^{\\langle t \\rangle}$, compute the prediction $\\hat{y}^{\\langle t \\rangle} = softmax(W_{ya} a^{\\langle t \\rangle} + b_y)$.\n",
    "3. Return $a^{\\langle t \\rangle}$ and $y^{\\langle t \\rangle}$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74b7c254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "\n",
    "def rnn_cell_forward(xt, a_prev, Wax, Waa, Wya, ba, by):\n",
    "    \"\"\"\n",
    "    Implements a single forward step of the RNN-cell\n",
    "    \n",
    "    Arguments:\n",
    "        xt -- your input data at timestep \"t\", numpy array of shape (n_x).\n",
    "        a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a)\n",
    "        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "        ba --  Bias, numpy array of shape (n_a, 1)\n",
    "        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "\n",
    "    Returns:\n",
    "        a_next -- next hidden state, of shape (n_a)\n",
    "        yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y)\n",
    "\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # compute next activation state\n",
    "    \n",
    "    a_next = np.tanh(np.dot(Wax, xt) + np.dot(Waa, a_prev) + ba)\n",
    "\n",
    "    # compute output of the current cell\n",
    "\n",
    "    z = np.dot(Wya, a_next) + by\n",
    "    yt_pred = softmax(z)\n",
    "    return a_next, yt_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3a0603",
   "metadata": {},
   "source": [
    "You can see an RNN as the repetition of the cell you've just built. If your input sequence of data is carried over 10 time steps, then you will copy the RNN cell 10 times. Each cell takes as input the hidden state from the previous cell ($a^{\\langle t-1 \\rangle}$) and the current time-step's input data ($x^{\\langle t \\rangle}$). It outputs a hidden state ($a^{\\langle t \\rangle}$) and a prediction ($y^{\\langle t \\rangle}$) for this time-step.\n",
    "\n",
    "\n",
    "<img src=\"rnn_forward.png\" style=\"width:800px;height:300px;\">\n",
    "\n",
    "\n",
    "\n",
    "Now, we are going to code the RNN forward propagation over the sequence\n",
    "\n",
    "**Steps**:\n",
    "1. Create a vector of zeros ($a$) that will store all the hidden states computed by the RNN.\n",
    "2. Initialize the \"next\" hidden state as $a_0$ (initial hidden state).\n",
    "3. Start looping over each time step, your incremental index is $t$ :\n",
    "    - Update the \"next\" hidden state and the cache by running `rnn_cell_forward`\n",
    "    - Keep a history of the parameters for the gradient update\n",
    "    - Store the \"next\" hidden state in $a$ ($t^{th}$ position) \n",
    "    - Store the prediction in y\n",
    "4. Return $a$, $y$ and history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35d73175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(x, a0, Wax, Waa, Wya, ba, by):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation of the recurrent neural network.\n",
    "\n",
    "    Arguments:\n",
    "        x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
    "        a0 -- Initial hidden state, of shape (n_a, m)\n",
    "        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "        ba -- Bias, numpy array of shape (n_a, 1)\n",
    "        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "\n",
    "    Returns:\n",
    "        a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
    "        y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
    "        history -- tuple of values needed for the backward pass, contains (list of caches, x)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Retrieve dimensions from the input data\n",
    "    print(x.shape)\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = Wya.shape\n",
    "\n",
    "    # Initialize \"a\" and \"y_pred\" with zeros\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    y_pred = np.zeros((n_y, m, T_x))\n",
    "\n",
    "    # Initialize a_next to a0\n",
    "    a_next = a0\n",
    "\n",
    "    # Initialize a list to store the caches\n",
    "    caches = []\n",
    "\n",
    "    # Loop over all time steps\n",
    "    for t in range(T_x):\n",
    "        # Extract the input at timestep t\n",
    "        xt = x[:, :, t]\n",
    "        \n",
    "        # Compute the next hidden state and the prediction using rnn_cell_forward\n",
    "        a_next, yt_pred = rnn_cell_forward(xt, a_next, Wax, Waa, Wya, ba, by)\n",
    "        \n",
    "        # Store the next hidden state in the \"a\" tensor\n",
    "        a[:, :, t] = a_next\n",
    "        \n",
    "        # Store the prediction in the \"y_pred\" tensor\n",
    "        y_pred[:, :, t] = yt_pred\n",
    "        \n",
    "        # Append the cache to the list\n",
    "        caches.append((a_next, xt, a0, Wax, Waa, Wya, ba, by))\n",
    "\n",
    "    # Store the list of caches and input in \"history\"\n",
    "    history = (caches, x)\n",
    "\n",
    "    return a, y_pred, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08d5213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run the rnn_forward for a sequence of 10 elements.\n",
    "inputs = np.array([[4, 0, 0, 0], [0, 8, 2, 0]]).transpose()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# what are the shapes of the weights and biases?\n",
    "# initialize them here\n",
    "a0 = np.zeros((3, 2))\n",
    "Wax = np.full((3, 4), 0.1)\n",
    "Waa = np.full((3, 3), 0.1)\n",
    "Wya = np.full((2, 3), 0.1)\n",
    "ba = np.full((3, 1), 0.1)\n",
    "by = np.full((2, 1), 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6cd3130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a, y_pred, history \u001b[38;5;241m=\u001b[39m \u001b[43mrnn_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWaa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWya\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mba\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma = \u001b[39m\u001b[38;5;124m\"\u001b[39m, a)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma.shape = \u001b[39m\u001b[38;5;124m\"\u001b[39m, a\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[17], line 23\u001b[0m, in \u001b[0;36mrnn_forward\u001b[0;34m(x, a0, Wax, Waa, Wya, ba, by)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Retrieve dimensions from the input data\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 23\u001b[0m n_x, m, T_x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     24\u001b[0m n_y, n_a \u001b[38;5;241m=\u001b[39m Wya\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Initialize \"a\" and \"y_pred\" with zeros\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "a, y_pred, history = rnn_forward(inputs, a0, Wax, Waa, Wya, ba, by)\n",
    "\n",
    "print(\"a = \", a)\n",
    "print(\"a.shape = \", a.shape)\n",
    "print()\n",
    "print(\"y_pred =\", y_pred)\n",
    "print(\"y_pred.shape = \", y_pred.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69eec4a",
   "metadata": {},
   "source": [
    "## Question 02\n",
    "\n",
    "Use PyTorch to implement an RNN cell and use it to generate random person names using the file `names.txt` as training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2f2052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Layers\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.o2o = nn.Linear(hidden_size, output_size)\n",
    "        self.initial_hidden = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def single_forward(self, input, hidden):\n",
    "        \"\"\"\n",
    "        Performs a single forward step for the RNN.\n",
    "        \n",
    "        Args:\n",
    "            input: Tensor of shape (batch_size, input_size)\n",
    "            hidden: Tensor of shape (batch_size, hidden_size)\n",
    "        \n",
    "        Returns:\n",
    "            output: Tensor of shape (batch_size, output_size)\n",
    "            hidden: Tensor of shape (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        # Concatenate input and hidden state\n",
    "        combined = torch.cat((input, hidden), dim=1)  # Shape: (batch_size, input_size + hidden_size)\n",
    "        \n",
    "        # Compute the next hidden state\n",
    "        hidden = self.tanh(self.i2h(combined))  # Shape: (batch_size, hidden_size)\n",
    "        \n",
    "        # Compute the output\n",
    "        output = self.softmax(self.o2o(hidden))  # Shape: (batch_size, output_size)\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def forward(self, input_line_tensor):\n",
    "        # YOUR CODE HERE\n",
    "        outputs = []\n",
    "\n",
    "        hidden = self.initial_hidden\n",
    "\n",
    "        for x_t in input_line_tensor:\n",
    "            output, hidden = self.single_forward(\n",
    "                x_t, hidden)\n",
    "            outputs.append(output)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7746d482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['francisco', 'maria', 'joão', 'leonor', 'afonso', 'matilde', 'tomás', 'carolina', 'duarte', 'alice', 'gabriel', 'benedita', 'santiago', 'beatriz', 'lourenço', 'mariana', 'rodrigo', 'francisca', 'miguel', 'sofia', 'martim', 'margarida', 'vicente', 'laura', 'mateus', 'camila', 'lucas', 'clara', 'guilherme', 'ana', 'salvador', 'inês', 'pedro', 'lara', 'gonçalo', 'madalena', 'dinis', 'diana', 'rafael', 'vitória', 'gustavo', 'constança', 'bernardo', 'eva', 'josé', 'mafalda', 'diogo', 'gabriela', 'manuel', 'bianca', 'tiago', 'valentina', 'vasco', 'joana', 'henrique', 'íris', 'simão', 'sara', 'diego', 'luana', 'antónio', 'ariana', 'david', 'letícia', 'daniel', 'rita', 'leonardo', 'júlia', 'enzo', 'carlota', 'xavier', 'carminho', 'eduardo', 'luísa', 'andré', 'ema', 'luís', 'olívia', 'isaac', 'helena', 'matias', 'yara', 'valentim', 'aurora', 'artur', 'yasmin', 'samuel', 'emma', 'kevin', 'teresa', 'frederico', 'mara', 'filipe', 'catarina', 'matheus', 'caetana', 'lorenzo', 'melissa', 'noah', 'alícia', 'alexandre', 'isabel', 'benjamim', 'noa', 'davi', 'marta', 'bryan', 'victória', 'sebastião', 'iara', 'ricardo', 'alana', 'joaquim', 'amélia', 'carlos', 'pilar', 'leandro', 'daniela', 'nicole']\n"
     ]
    }
   ],
   "source": [
    "# all letters to work with the Portuguese names from the train file\n",
    "all_letters = 'abcdefghijklmnopqrstuvxwyzáàãçéêíóõôú'\n",
    "n_letters = len(all_letters) + 2 # Plus BOS and EOS marker\n",
    "\n",
    "# One-hot matrix of first to last letters (not including EOS) for input\n",
    "# Adds a BOS token as the first token\n",
    "def inputTensor(line):\n",
    "    tensor = torch.zeros(len(line)+1, 1, n_letters)\n",
    "    tensor[0][0][-2] = 1  # BOS\n",
    "    for letter_idx in range(len(line)):\n",
    "        letter = line[letter_idx]\n",
    "        tensor[letter_idx+1][0][all_letters.find(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# LongTensor of second letter to end (EOS) for target\n",
    "def targetTensor(line):\n",
    "    letter_indexes = [all_letters.find(line[l_idx]) for l_idx in range(len(line))]\n",
    "    letter_indexes.append(n_letters - 1) # EOS\n",
    "    return torch.LongTensor(letter_indexes).unsqueeze(-1)\n",
    "\n",
    "# Read the list of names\n",
    "with open(\"names.txt\") as file:\n",
    "    names = [name.lower().strip() for name in file.readlines()]\n",
    "\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9b02b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 673/1000 [06:26<03:07,  1.74it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m         all_losses\u001b[38;5;241m.\u001b[39mappend(loss_agg)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_losses\n\u001b[0;32m---> 40\u001b[0m all_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_data, num_epochs, optimizer, criterion)\u001b[0m\n\u001b[1;32m     27\u001b[0m     loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\n\u001b[1;32m     28\u001b[0m         [ criterion(o_i, t_i) \u001b[38;5;28;01mfor\u001b[39;00m o_i ,t_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(outputs, target_line_tensor)]\n\u001b[1;32m     29\u001b[0m     )\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     31\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 32\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     loss_agg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m/\u001b[39m input_line_tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     36\u001b[0m all_losses\u001b[38;5;241m.\u001b[39mappend(loss_agg)\n",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages/torch/optim/adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    160\u001b[0m         group,\n\u001b[1;32m    161\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m         state_steps)\n\u001b[0;32m--> 168\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages/torch/optim/adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 318\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages/torch/optim/adam.py:441\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    443\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from random import shuffle, choices\n",
    "\n",
    "train_data = [ (inputTensor(name),targetTensor(name)) for name in names ]\n",
    "hidden_size = 128\n",
    "num_epochs = 1000\n",
    "criterion = nn.NLLLoss()\n",
    "learning_rate = 5e-4\n",
    "model = RNN(n_letters, hidden_size, n_letters)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "def train(model, train_data, num_epochs, optimizer, criterion ):\n",
    "    all_losses = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "        shuffle(train_data)\n",
    "        loss_agg = 0\n",
    "\n",
    "        for input_line_tensor, target_line_tensor in train_data:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model.forward(input_line_tensor)\n",
    "\n",
    "            loss = torch.stack(\n",
    "                [ criterion(o_i, t_i) for o_i ,t_i in zip(outputs, target_line_tensor)]\n",
    "            ).sum()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_agg += loss.item()/ input_line_tensor.size(0)\n",
    "\n",
    "        all_losses.append(loss_agg)\n",
    "\n",
    "    return all_losses\n",
    "\n",
    "all_losses = train(\n",
    "    model=model,\n",
    "    train_data=train_data,\n",
    "    num_epochs=num_epochs,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84af223f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's inpsect the training loss\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655eb366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from a category and starting letter\n",
    "def sampling_names(start_string='', method=\"sample\", kk=1, max_length=20, vocabulary=all_letters):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        hidden = model.initial_hidden\n",
    "\n",
    "        # process each letter of start_string, including BOS\n",
    "        output_name = start_string\n",
    "        input = inputTensor(start_string)\n",
    "        if input.shape[0] > 1:\n",
    "            for input_t in input[:-1].split(1):\n",
    "                _, hidden = model.single_forward(input_t[0], hidden)\n",
    "\n",
    "        # start sampling from the last letter of start_string\n",
    "        for _ in range(max_length):\n",
    "            output, hidden = model.single_forward(input[-1], hidden)\n",
    "            # top-k sampling\n",
    "            if method == \"top_k\":\n",
    "                # YOUR CODE HERE\n",
    "                # topi = \n",
    "                raise NotImplementedError\n",
    "            # random sampling\n",
    "            elif method == \"sample\":\n",
    "                # YOUR CODE HERE\n",
    "                # topi = \n",
    "                raise NotImplementedError\n",
    "            # stop sampling if EOS\n",
    "            if topi == n_letters - 1:\n",
    "                break\n",
    "            # ignore BOS\n",
    "            elif topi == n_letters - 2:\n",
    "                continue\n",
    "            # add letter to output_name\n",
    "            else:\n",
    "                letter = vocabulary[topi]\n",
    "                output_name += letter\n",
    "            # use the sampled letter as the next input\n",
    "            input = inputTensor(letter)\n",
    "\n",
    "        return output_name\n",
    "\n",
    "starting_string = ''\n",
    "print(\"--random sampling--\")\n",
    "# generate 3 random names with random sampling\n",
    "# note how it is only copying the names from the training set\n",
    "for _ in range(3):\n",
    "    print(sampling_names(starting_string, method=\"sample\"))\n",
    "print(\"\\n--top-k sampling--\")\n",
    "# generate 3 random names with random sampling\n",
    "for _ in range(3):\n",
    "    print(sampling_names(starting_string, method=\"top_k\", kk=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de9f503",
   "metadata": {},
   "source": [
    "## Question 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d3482bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from datasets) (3.10.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from datasets) (0.24.6)\n",
      "Requirement already satisfied: packaging in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from aiohttp->datasets) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from huggingface-hub>=0.23.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/joaocosta/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9548318",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.tokenization_utils_base because of the following error (look up to see its traceback):\nlibssl.so.10: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages/transformers/utils/import_utils.py:1184\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:843\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:75\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available():\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtokenizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AddedToken\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtokenizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Encoding \u001b[38;5;28;01mas\u001b[39;00m EncodingFast\n",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages/tokenizers/__init__.py:79\u001b[0m\n\u001b[1;32m     76\u001b[0m     CONTIGUOUS \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontiguous\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     80\u001b[0m     Tokenizer,\n\u001b[1;32m     81\u001b[0m     Encoding,\n\u001b[1;32m     82\u001b[0m     AddedToken,\n\u001b[1;32m     83\u001b[0m     Regex,\n\u001b[1;32m     84\u001b[0m     NormalizedString,\n\u001b[1;32m     85\u001b[0m     PreTokenizedString,\n\u001b[1;32m     86\u001b[0m     Token,\n\u001b[1;32m     87\u001b[0m )\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decoders\n",
      "\u001b[0;31mImportError\u001b[0m: libssl.so.10: cannot open shared object file: No such file or directory",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset_builder\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 5\u001b[0m ds_builder \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimdb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m ds_builder\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mfeatures\n",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages/datasets/load.py:1904\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1889\u001b[0m \u001b[38;5;66;03m# Instantiate the dataset builder\u001b[39;00m\n\u001b[1;32m   1890\u001b[0m builder_instance: DatasetBuilder \u001b[38;5;241m=\u001b[39m builder_cls(\n\u001b[1;32m   1891\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1892\u001b[0m     dataset_name\u001b[38;5;241m=\u001b[39mdataset_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1902\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[1;32m   1903\u001b[0m )\n\u001b[0;32m-> 1904\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_use_legacy_cache_dir_if_possible\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1906\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\n",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages/datasets/builder.py:642\u001b[0m, in \u001b[0;36mDatasetBuilder._use_legacy_cache_dir_if_possible\u001b[0;34m(self, dataset_module)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_use_legacy_cache_dir_if_possible\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_module: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetModule\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;66;03m# Check for the legacy cache directory template (datasets<3.0.0)\u001b[39;00m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_legacy_relative_data_dir \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 642\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_legacy_cache2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_module\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_legacy_cache() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     )\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_cache_dir()\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_dir\n",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages/datasets/builder.py:486\u001b[0m, in \u001b[0;36mDatasetBuilder._check_legacy_cache2\u001b[0;34m(self, dataset_module)\u001b[0m\n\u001b[1;32m    484\u001b[0m namespace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepo_id\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepo_id \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m patch\u001b[38;5;241m.\u001b[39mobject(Pickler, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_legacy_no_dict_keys_sorting\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 486\u001b[0m     config_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mHasher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhash\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_files\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28mhash\u001b[39m \u001b[38;5;241m=\u001b[39m _PACKAGED_DATASETS_MODULES_2_15_HASHES\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    489\u001b[0m     dataset_module\u001b[38;5;241m.\u001b[39mbuilder_configs_parameters\u001b[38;5;241m.\u001b[39mmetadata_configs\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_configs_parameters\u001b[38;5;241m.\u001b[39mmetadata_configs\n\u001b[1;32m    491\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages/datasets/fingerprint.py:188\u001b[0m, in \u001b[0;36mHasher.hash\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhash\u001b[39m(\u001b[38;5;28mcls\u001b[39m, value: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mhash_bytes(\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages/datasets/utils/_dill.py:109\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Pickle an object to a string.\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m file \u001b[38;5;241m=\u001b[39m BytesIO()\n\u001b[0;32m--> 109\u001b[0m \u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages/datasets/utils/_dill.py:103\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, file)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file):\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Pickle an object to a file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     \u001b[43mPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecurse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages/dill/_dill.py:420\u001b[0m, in \u001b[0;36mPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj): \u001b[38;5;66;03m#NOTE: if settings change, need to update attributes\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     logger\u001b[38;5;241m.\u001b[39mtrace_setup(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 420\u001b[0m     \u001b[43mStockPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/pickle.py:487\u001b[0m, in \u001b[0;36m_Pickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframer\u001b[38;5;241m.\u001b[39mstart_framing()\n\u001b[0;32m--> 487\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(STOP)\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframer\u001b[38;5;241m.\u001b[39mend_framing()\n",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages/datasets/utils/_dill.py:70\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj_type \u001b[38;5;129;01mis\u001b[39;00m FunctionType:\n\u001b[1;32m     69\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torchdynamo_orig_callable\u001b[39m\u001b[38;5;124m\"\u001b[39m, obj)\n\u001b[0;32m---> 70\u001b[0m \u001b[43mdill\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_persistent_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_persistent_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages/dill/_dill.py:414\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    412\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt pickle \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: attribute lookup builtins.generator failed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m GeneratorType\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(msg)\n\u001b[0;32m--> 414\u001b[0m \u001b[43mStockPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_persistent_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages/dill/_dill.py:1217\u001b[0m, in \u001b[0;36msave_module_dict\u001b[0;34m(pickler, obj)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_dill(pickler, child\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m pickler\u001b[38;5;241m.\u001b[39m_session:\n\u001b[1;32m   1215\u001b[0m         \u001b[38;5;66;03m# we only care about session the first pass thru\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m         pickler\u001b[38;5;241m.\u001b[39m_first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m     \u001b[43mStockPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpickler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1218\u001b[0m     logger\u001b[38;5;241m.\u001b[39mtrace(pickler, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# D2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/pickle.py:971\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(MARK \u001b[38;5;241m+\u001b[39m DICT)\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemoize(obj)\n\u001b[0;32m--> 971\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_setitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages/datasets/utils/_dill.py:74\u001b[0m, in \u001b[0;36mPickler._batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_batch_setitems\u001b[39m(\u001b[38;5;28mself\u001b[39m, items):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_legacy_no_dict_keys_sorting:\n\u001b[0;32m---> 74\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_setitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# Ignore the order of keys in a dict\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;66;03m# Faster, but fails for unorderable elements\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/pickle.py:1002\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m   1000\u001b[0m     k, v \u001b[38;5;241m=\u001b[39m tmp[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1001\u001b[0m     save(k)\n\u001b[0;32m-> 1002\u001b[0m     \u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1003\u001b[0m     write(SETITEM)\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;66;03m# else tmp is empty, and we're done\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages/datasets/utils/_dill.py:64\u001b[0m, in \u001b[0;36mPickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules:\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(obj_type, \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPreTrainedTokenizerBase\u001b[49m):\n\u001b[1;32m     65\u001b[0m             pklregister(obj_type)(_save_transformersPreTrainedTokenizerBase)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Unwrap `torch.compile`-ed functions\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages/transformers/utils/import_utils.py:1174\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1174\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1175\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/IST_DL21_Env/lib/python3.8/site-packages/transformers/utils/import_utils.py:1186\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1187\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1188\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1189\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.tokenization_utils_base because of the following error (look up to see its traceback):\nlibssl.so.10: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset_builder\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "ds_builder = load_dataset_builder(\"imdb\")\n",
    "ds_builder.info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40ac1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "\n",
    "print(f'Number of training examples: {len(dataset[\"train\"])}')\n",
    "print(f'Number of test examples: {len(dataset[\"test\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e4ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a vocabulary of the top K words in the dataset, without stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_vocab(dataset, vocab_size=1024):\n",
    "    counter = Counter()\n",
    "    for example in dataset:\n",
    "        counter.update([word for word in word_tokenize(example['text'].lower())])\n",
    "    vocab = counter.most_common(vocab_size)\n",
    "    return [word for word, _ in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cbbc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pytorch dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dataset, vocab, split=\"train\", first_n_words=128):\n",
    "        self.dataset = dataset\n",
    "        self.vocab = vocab\n",
    "        self.unk_idx = 0\n",
    "        self.pad_idx = 1\n",
    "        self.first_n_words = first_n_words\n",
    "        # use a special dict that maps words to indices and if the word is not\n",
    "        # in the vocab, the special dict automatically returns the index of the\n",
    "        # unknown token\n",
    "        self.word2idx = defaultdict(lambda: self.unk_idx)\n",
    "        self.word2idx['<pad>'] = self.pad_idx\n",
    "        # add the words to the dict, starting from index 2 (0 is for the unknown token,\n",
    "        # 1 is for the padding token)\n",
    "        self.word2idx.update({word: idx + 2 for idx, word in enumerate(self.vocab)})\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        \n",
    "        self.split = split\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.split == \"train\":\n",
    "            return len(self.dataset)\n",
    "        else:\n",
    "            return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.dataset[idx]\n",
    "        text = word_tokenize(example['text'])\n",
    "        text = [self.word2idx[word] for word in text][:self.first_n_words]\n",
    "        return torch.LongTensor(text), example['label']\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        This function takes a list of tokens, already as the index of a vocabulary\n",
    "        and turns it into a tensor, with padding where appropriate.\n",
    "        The padding index is stored in self.pad_idx\n",
    "\n",
    "        Args:\n",
    "            batch (list): a list of tuples,\n",
    "                where each tuple has the tokens for one sequence in position 0\n",
    "                and the label for that sequence in position 1.\n",
    "\n",
    "        Returns:\n",
    "            torch.tensor: return the padded tensor and the labels in tensor form.\n",
    "                The padded tensor is of size [batch_size, max_len],\n",
    "                the labels tensor is [batch_size]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        # YOUR CODE HERE\n",
    "        # get the max length of the sentences in the batch\n",
    "        \n",
    "        # pad all the sentences to the max length\n",
    "\n",
    "        # convert the lists to tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bfa34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout=0.3):\n",
    "\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        raise NotImplementedError\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # pass the tokens through the embedding layer\n",
    "\n",
    "        # apply dropout to the embeddings\n",
    "\n",
    "\n",
    "        # pass it simply through the LSTM initialized above\n",
    "        # (you don't need a for loop, nn.LSTM takes care of that for you)\n",
    "\n",
    "\n",
    "        # apply dropout to the output\n",
    "\n",
    "\n",
    "        # Take the output of the last token of the sequence\n",
    "\n",
    "\n",
    "        # pass it through the fully connected\n",
    "        # return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170a444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c768bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_daloader, optimizer, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for text_tensors, targets in tqdm(train_daloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(text_tensors).squeeze(1)\n",
    "\n",
    "        loss = criterion(predictions, targets.float())\n",
    "\n",
    "        acc = binary_accuracy(predictions, targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(train_daloader), epoch_acc / len(train_daloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39297ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, eval_dataloader, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for text_tensors, targets in tqdm(eval_dataloader):\n",
    "\n",
    "            predictions = model(text_tensors).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions, targets.float())\n",
    "\n",
    "            acc = binary_accuracy(predictions, targets)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(eval_dataloader), epoch_acc / len(eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aaef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "\n",
    "# set the seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "VOCAB_SIZE = 1024\n",
    "FIRST_N_WORDS = 128\n",
    "BATCH_SIZE = 64\n",
    "INPUT_DIM = VOCAB_SIZE + 2  # +2 for the unknown and padding tokens\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT_RATE = 0.3\n",
    "LR_RATE = 5e-5\n",
    "\n",
    "vocab = get_vocab(dataset['train'], vocab_size=1024)\n",
    "# use 80% of the training set for training and 20% for validation\n",
    "train_size = int(0.8 * len(dataset['train']))\n",
    "valid_size = len(dataset['train']) - train_size\n",
    "# split the training set\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset['train'], [train_size, valid_size])\n",
    "# create the pytorch datasets\n",
    "train_dataset = IMDBDataset(train_dataset, vocab, split=\"train\", first_n_words=FIRST_N_WORDS)\n",
    "valid_dataset = IMDBDataset(valid_dataset, vocab, split=\"valid\")\n",
    "test_dataset = IMDBDataset(dataset['test'], vocab, split=\"test\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=valid_dataset.collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=test_dataset.collate_fn)\n",
    "\n",
    "model = RNN(INPUT_DIM,\n",
    "            EMBEDDING_DIM,\n",
    "            HIDDEN_DIM,\n",
    "            OUTPUT_DIM,\n",
    "            dropout=DROPOUT_RATE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR_RATE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Train for 5 epochs\n",
    "for epoch in range(5):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_loader, criterion)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2248ea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f'\\t Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb39e81b",
   "metadata": {},
   "source": [
    "Let's do some changes to use a bidirectional LSTM and implement some other improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7662d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim//2 if bidirectional else hidden_dim,\n",
    "                            num_layers=n_layers,\n",
    "                            bidirectional=bidirectional,\n",
    "                            batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        #text: [batch size, sent len]\n",
    "        #embedded: [batch size, sent len, emb dim]\n",
    "\n",
    "        # output: [batch size, sent len, hid dim * num directions]\n",
    "        # hidden: [num layers * num directions, batch size, hid dim]\n",
    "        # cell: [num layers * num directions, batch size, hid dim]\n",
    "\n",
    "        #hidden: [batch size, hid dim * num directions]\n",
    "        if self.bidirectional:\n",
    "            #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37cf56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1024\n",
    "FIRST_N_WORDS = 256\n",
    "BATCH_SIZE = 64\n",
    "INPUT_DIM = VOCAB_SIZE + 2  # +2 for the unknown and padding tokens\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 1\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT_RATE = 0.3\n",
    "LR_RATE = 5e-5\n",
    "EMBEDDING_DIM = 128\n",
    "\n",
    "model = RNN(INPUT_DIM,\n",
    "            EMBEDDING_DIM,\n",
    "            HIDDEN_DIM,\n",
    "            OUTPUT_DIM,\n",
    "            N_LAYERS,\n",
    "            BIDIRECTIONAL,\n",
    "            DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea38e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LR_RATE)\n",
    "\n",
    "# Train for 5 epochs\n",
    "for epoch in range(5):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_loader, criterion)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6adb7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f'\\t Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSLabPreprocess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a96707b1aefed7c05ab8288ccb93c4e2b95de2651b5fe43ac3cd3e321955411"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
